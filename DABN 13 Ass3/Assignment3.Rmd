---
title: "Assignment 3"
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Preamble: Prostate specific antigen
Prostate specific antigen (PSA) might be know to you in the context of PSA tests, a widely used albeit somewhat controversial method to detect prostate cancer in middle-aged men. The dataset we are using in this lab contains measures of PSA levels in men shortly before they received prostate cancer treatment in the form of a surgical removal of their prostate. Additional variables are measures associated with the existing prostate cancer (log cancer volume Gleason score, percent of Gleason scores 4 or 5, seminal vesicle invasion, capsular penetration) as well as cancer-unrelated variables that are suspected to affect PSA levels (age, amount of benign prostatic hyperplasia, log prostate weight). 

## Part one: Mechanics
In this initial part, we are going to manually conduct the grid search over tuning parameter values that glmnet automatically provides us with when we fit the lasso or ridge regression. We are going to find the optimal tuning parameter for ridge regression since this allows us even to program the ridge regression estimator manually.
The goal with this part is that you by building the functions on your own will gain a greater understanding in what is presented in the lectures, and hopefully a deeper understanding.

Before conducting the first task, let us load some data.
```{r }
library(dplyr)
prostate <- read.table('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data')
train_index  = prostate$train==T
prostate <- prostate %>% dplyr::select(-train)
prostate.train <- prostate[train_index,]
```

### Task 1a)
First we are going to demean all variables. In addition to that, we also want to standardize the sample variance of all predictors to one. 
We will use `preProcess()` function in the `caret` library. The reason is that we want to store the parameter used for scaling. Use the `scaled.param` to scale (demean and standardize) the training data.  Read `help(preProcess)` on how to demean and scale the data.
```{r }
library(caret)
scaled.param <- preProcess(prostate.train, method = c("center", "scale"))
prostate.train <- predict(scaled.param, prostate.train)
```
### Task 1b)
Since we are estimating ridge regression manually in this part, we need to make sure that the features of our dataset are transformed into numerical variables. Use the `model.matrix()` command to create such a matrix,`X`, of *all* predictors. Here, the intercept should not be included in the `X`  since the data is demeaned (centered). the data in the next step. Use a suitable specification for this purpose when expressing the formula in `model.matrix()'
Additionally, extract our outcome of interest from the data into a vector `y`.
```{r}
X <- model.matrix(lpsa ~ 0 + lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate.train)
y <- as.matrix(prostate.train[,"lpsa"])
```
### Task 1c)
Demeaning all variables (`X` and `y`) has the convenient consequence that we do not need to include an intercept into our model. This is also very desirable in our manual implementation of ridge regression since we do not want to regularize the intercept. But WHY is it nonsensical to regularize the intercept? Please explain!
Additionally, please explain why it is most often a good idea to standardize the variance of all predictors to one.
``` {r}
intercept_regul1c <- "Penalization of the intercept would make the procedure depend on the origin
chosen for Y, adding a constant to each of the targets yi would not simply result in a shift of the predictions by the same amount constant. The bias should not be shrunk, because the ridge regression uses the bias to influence the variances."
standardize_varc <- "Two independent variables with different scales will have different contributions to the penalized terms, because the penalized term is a sum of squares of all the coefficients. To avoid such kind of problems, the independent variables are centered and scaled to one in order to have variance 1."

```
### Task 1d)
Now we are in the position to write a function that estimates the ridge regression coefficient estimates for us. Write such a function `beta.ridge` using matrix operations in R. The function inputs should have predictors `X`, outcome `y` and the value of the tuning parameter `lambda` as inputs. 
```{r}
beta.ridge <- function(X, y,lambda){
I <- diag(1, dim(X)[2])
  
beta <- solve(t(X)%*%X+lambda*I)%*%t(X)%*%y

return(beta)
}
```
### Task 1e)
In order to measure model generality, we also want to obtain the degrees of freedom of a particular ridge regression with shrinkage parameter $\lambda$. 
Write the function `dfs.ridge` using `X` and `lambda.seq` that derives these degrees of freedom for the ridge problem for a sequence of $\lambda$'s.
```{r }
dfs.ridge <- function(X, lambda.seq){
  n.lambda <- length(lambda.seq)
  dfs <- rep(0, n.lambda)
   
  for(i in 1:n.lambda){
    I <- diag(1, nrow = dim(X)[2], ncol = dim(X)[2])
    dfs[i] <- sum(diag(X%*%solve((t(X)%*%X)+lambda.seq[i]*I)%*%t(X))) 
 }
  return(dfs)
}


```
### Task 1f)
What does regularization, expressed by the value of the tuning parameter $\lambda$ to the estimated coefficients of ridge regression. The functions above allow you to investigate this question and to answer the following questions:

1. What is the relation between $RSS$ and $\lambda$ and why is it that way?
2. How does that show that we can't use the training error for selecting the hyperparameters?

``` {r }
lambda.grid <- 10^(seq(0,3, length=100))
all.beta <- matrix(NA, nrow=dim(X)[2], ncol=length(lambda.grid))
all.dfs  <- dfs.ridge(X, lambda.grid)
all.RSS  <- all.dfs
for (i in 1:length(lambda.grid)) {
  all.beta[,i] = beta.ridge(X, y, lambda.grid[i])
  all.RSS[i] = mean((y-X%*%all.beta[,i])^2)
}
library(ggplot2)
library(reshape2)
fig1 <- ggplot(data.frame(lambda = lambda.grid,
                  dfs = all.dfs),aes(x=lambda, y=dfs)) + 
  geom_line() +
  scale_x_log10()
print(fig1)
fig2 <- ggplot(data.frame(lambda = lambda.grid,
                  RSS = all.RSS),aes(x=lambda, y=RSS)) + 
  geom_line() +
  scale_x_log10()
print(fig2)
rownames(all.beta) <-colnames(X)
colnames(all.beta) <- lambda.grid
all.beta.long = melt(all.beta, id = variable)
names(all.beta.long) <- c("Variable", "lambda", "Coefficient_value")
fig3 <- ggplot(all.beta.long, aes(x=lambda)) + 
  geom_line(aes(y=Coefficient_value, color=Variable)) + 
  scale_x_log10()
ex1f_relation_between_RSS_lambda <-"1. When increasing lambda, the penalty will also increase. When the penalty goes up, the least squres estimate goes down, leading to a higher RSS. 2. When using the training error, we do not have the random error term, therefore, when setting the lambda to 0, we simply end up with the OLS regression."
```
### Task 1g)
Below we plot what is known as the ridge path. Explain what the figure displays.
```{r }
rownames(all.beta) <-colnames(X)
colnames(all.beta) <- lambda.grid
all.beta.long = melt(all.beta, id = variable)
names(all.beta.long) <- c("Variable", "lambda", "Coefficient_value")
fig <- ggplot(all.beta.long, aes(x=lambda)) + 
  geom_line(aes(y=Coefficient_value, color=Variable)) + 
  scale_x_log10()
print(fig)
ex1g_fig_what_do_I_see <- "The figure shows the ridge coefficient estimates for the Prostate specific antigen example, plotted as functions of df(λ), the effective degrees of freedom implied by the penalty λ. When λ grows larger, the squares estimate go towards zero, and when they are zero you end up with the OLS regression."

```

### Task 1h)

We have discussed quite extensively that increasing the value of the shrinkage parameter $\lambda$ will draw the vector of estimated slope coefficients further to zero. However, when I have a look at the ridge path in Task 1g, I see at least one slope coefficient whose values temporarily *increases* as we increase $\lambda$. How is this possible?

``` {r }

explain_ridge_path_mystery1h <- "In the penalty, the lambda is multiplying every single value inside the beta's. Some betas can level each other out, because they depend on each other. For instance when when one beta is -10 and another one is 10, it becomes zero. However, some values cannot be leveled out, leading to the slope go up when another one goes down. Because lambda goes over every value of the beta, it can lead to a temporary *increase* of the slope when we increase lambda, before becoming O (which is when lambda is infinite)"

```


### Task 1i)
So far we have only looked at the training error (RSS). However, using this measure to choose our tuning parameter value will surely lead to overfitting.  As a remedy, we will add the optimism of training error, $\omega$, in order to arrive at an estimate of in-sample error, $E_y[Err_in] $. In Lecture 5, we encountered this estimate as *Mallow's $C_p$*. Since ridge regression requires us to specify a tuning parameter, $\widehat{E_y[Err_{in}]}(\lambda)$ is a function of $\lambda$.
Write a function that for each $\lambda$ in `lambda.seq` computes estimated $\widehat{E_y[Err_{in}]}(\lambda)$ (hint the formula is in the lecture slides), and use $\hat{\sigma}^2 = \frac{RSS}{n}$  as an estimate of $\sigma^2$.
```{r }


err.ridge <- function(X, y, lambda.seq){
 n.lambda <- length(lambda.seq)
 err <- rep(0, n.lambda)
 n <- dim(X)[1]
 p <- dim(X)[2]
 OLS <- lm(lpsa ~ .-1, data = prostate.train)
 sigma2 <- var(resid(OLS))
 
RSS1 <- rep(0, length(all.RSS))

    for(i in 1:n.lambda){
      RSS1[i] <- mean((y-X%*%beta.ridge(X, y,lambda.seq[i]))^2)
      err[i] <- RSS1[i]+(2/n)*sigma2*dfs.ridge(X, lambda.seq[i])
    
  }
  return(err)
}

```
### Task 1j)
Use the function `err.ridge` to compute the estimated in-sample error as a function of degrees of freedom. Here we can use the estimated in-sample error for estimating the hyperparameter. From the $\widehat{E_y[Err_{in}]}(\lambda)$  select $\lambda$  in  `lambda.est`
```{r }
lambda.seq <- seq(0,5,length.out = 100)
err <- err.ridge(X,y, lambda.seq)
fig <- ggplot(data.frame(err=err, lambda=lambda.seq), aes(x=lambda)) + 
  geom_line(aes(y=err)) 
print(fig)
lambda.est <- lambda.seq[(which.min(err))]
```


## Part two: glmnet
Now that you implemented your own ridge regression we are going learn how one can fit it in R using `glmnet`, the go-to library for lasso and ridge. In this part, you will fit both ridge and lasso on the data and familiarize yourself with the various functions in `glmnet`.
First we load the library
```{r  }
library(glmnet)
```
### Task 2a)
Fit ridge regression in `glmnet` for an equally spaced grid of 400 tuning parameter values $\lambda$ between 0 and 20 .
To do this, read the help section for `glmnet` and estimate the ridge.regression model with 
```{r  }
lambda.seq <- seq(from = 0, to = 20, length.out = 400)
glm.obj <- glmnet(X, y, lambda=lambda.seq, alpha = 0)
plot(glm.obj,xvar='lambda')
```

### Task 2b)
In `glmnet` one often fits the parameters using n-fold cross-validation. This is done by the function `cv.glmnet`. Here you should find a $\lambda$ for the ridge regression problem through 10-fold cross-valdiation with mean squared error as loss function. Read the first five pages of the introduction [https://rdrr.io/cran/glmnet/f/inst/doc/glmnet.pdf](intro glm). Also note that in glmnet one is choosing the $\lambda$ that minimizes $\frac{1}{2N} RSS + \frac{\lambda}{2}||\beta||_2^2$ which means that $\lambda$ here corresponds to $\frac{\lambda}{n}$ in task one.
For `lambda.seq` use 100 equally spaced values between 0 and 2.
```{r }
set.seed(5)
lambda.seq <- seq(from = 0, to = 2, length.out = 100)
glm.cv.obj.ridge <- cv.glmnet(X, y, alpha=0,lambda = lambda.seq, type.measure = "mse",intercept=FALSE)
plot(glm.cv.obj.ridge)
ridge.lambda.min <- glm.cv.obj.ridge$lambda.min
ridge.coeff.lambda.min <- coef(glm.cv.obj.ridge, s = 'lambda.min')
fig2c_what_does_the_vertical_line_mean<- "The vertical lines ar the lambda.min and the lambda.1se. lambda.min is the value of λ that gives minimum mean cross-validated error, while lambda.1se is the value of λ that gives the most regularized model such that the cross-validated error is within one standard error of the minimum."  
fig2c_what_does_the_bars_mean<- "The bars represent the upper and lower standard deviation curve, along the lambda sequence, which are called the error bars."  
```
### Task 2d)
It is beyond the scope of this course to derive a solver for the lasso problem. But one can again use  `glmnet` to find the parameters.
We will go through the same step for the ridge regression but for lasso. First we fit the parameters on the full training data.

```{r }
glm.obj.lasso <- glmnet(X, y, alpha=1 , lambda = lambda.seq)
plot(glm.obj.lasso, xvar='lambda')
```
### Task 2e) 
To find the hyperparameter, $\lambda$, we again use 10-fold cross-validation with the help of `cv.glmnet` .
```{r }
set.seed(5)
glm.cv.obj.lasso <- cv.glmnet(X, y,alpha=1, lambda = lambda.seq, type.measure = "mse",intercept=FALSE)
plot(glm.cv.obj.lasso)
lasso.lambda.min <- glm.cv.obj.lasso$lambda.min
lasso.coeff.lambda.min <- coef(glm.cv.obj.lasso, s = 'lambda.min')
lasso.coeff.lambda.se <- coef(glm.cv.obj.lasso, s = 'lambda.1se')
```
## Final part: model performance
### Task 3a) 
Now we are going to see how well our models perform on new data (i.e. test data). However, first we are going to center and scale the data using our old object `scaled.param`. The reason we don't scale with new data is that one does not have this information when doing the prediction (you can only demean data that you observed), if we would have done this we would in some sense be cheating.
```{r  }
prostate.test <- prostate[train_index==F,]
prostate.test <- predict(scaled.param, prostate.test)
X.test <- model.matrix(lpsa ~ 0 + lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = prostate.test)
y.test <- as.matrix(prostate.test[,"lpsa"])
```
### Task 3b) 
Now you are going to compare the mean RSS for the model we fitted above. This we will do by using the predict function `predict` which you can read about with `help("predict.glmnet")`. Which model fitting method preformed best?
``` {r }
beta_ols <- solve(t(X)%*%X, t(X)%*%y)
yhat_ols <- X.test%*%beta_ols
yhat_ridge.min <- predict(glm.cv.obj.ridge, newx=X.test, s="lambda.min")
yhat_lasso.min <- predict(glm.cv.obj.lasso, newx=X.test, s = "lambda.min" )
yhat_lasso.se <- predict(glm.cv.obj.lasso, newx=X.test, s = "lambda.1se" )
RSS.ols <- mean((y.test - yhat_ols)^2)
RSS.lasso.min <-  mean((y.test-yhat_lasso.min)^2)
RSS.ridge.min <-  mean((y.test-yhat_ridge.min)^2)
RSS.lasso.se <-  mean((y.test-yhat_lasso.se)^2)
Which_was_the_best <- "Lasso.se has the best fit, because it has the smallest RSS" 
```