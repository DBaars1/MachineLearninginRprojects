---
title: "Assignment 1"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

This lab comes in the form of an R Markdown document which you are supposed to fill in. All instances that require you input are marked by "??". Please replace that with the corresponding code for a given task. Additionally, you need to uncomment all commented (`#`) lines in the R code chunks below in order to make the script work. Moreover, note the following:

* Often, we have specified names for objects that you are supposed to create. Use them. Do not come up with any object names of your own.
* Tasks that require you to write a function will provide you with function name, all inputs to the function as well as the function output. Your task is to write operations within the function that work with these given parameters. 
* At times, you will be asked to use a specific function to complete a task. If this is the case, please follow instructions to the point. Do not implement any alternative  way of performing the task.
* Sometimes, you might have questions concerning the use of a specific R command. Please approach this situation as in a real-life programming situation: First, use the R help files to find an answer. If unsuccessful, use Google. If still unsuccessful, post your question on Slack.
* Please write text answers into the corresponding string variables.


## Part one: Least squares regression mechanics

In this section, you are supposed to create your own algorithms for some standard statistics of a fitted linear regression model from scratch. You are only allowed to use the operations
```{r ,echo=FALSE}
cat("solve, %*%, /, \n")
```
to manually produce results otherwise produced by the lm() command.

# Load the data set

We will be working with ` Guns.dta`, a Stata dataset containing yearly US state data between 1977 and 1999 of three different crime rates, a number of additional state characteristics, as well as an indicator for the existence of a "shall-carry" law that allows citizens to obtain a permission to wear concealed handguns. In the following, you will fit a simple predictive model for state-wide violent crime rates.

To begin with, use the read.dta command in the "foreign" package to load ` Guns.dta`


```{r , echo=T}
# ?? (load "foreign" package here)
# ?? (optionally set working directory)
library(foreign)
library(tidyverse)
library(haven)
guns.data <- read.dta("Guns-1.dta")
setwd(dir = "/Users/Diederik/Desktop/DABN 13 Ass1/")


```

# Task 1a)
First, use the names()-command to report the variable names of `guns.data`
Then, construct an ` X`-matrix containing the columns for an intercept, the logarithm of state population, average per capita income, shall-carry law in effect, as well as the logarithmic rates for murder and robberies. Additionally, create a `y`-vector containing the log violent crime rate (for the state that year).
```{r , echo=T}

names(guns.data)
guns.data <- guns.data %>%
  mutate(Intercept = 1, 
         pop.l = log10(pop),
         mur.l = log10(mur),
         rob.l = log10(rob), 
         vio.l = log10(vio))

X <- cbind(guns.data$Intercept,
  guns.data$pop.l,
  guns.data$avginc,
  guns.data$shall,
  guns.data$mur.l,
  guns.data$rob.l)
  
y <- guns.data$vio.l


```


# Task 1b)
Build a function that uses ` X` and ` y` as inputs and returns the least squares
estimate $\hat{\beta}$ of the slope coefficients on ` X`. Here you are only allowed to use matrix and scalar operations.
## Beta hat = (X^TX)^-1 X^Ty
```{r , echo=T}
estimate.beta <- function(X, y){
 beta <- solve((t(X)%*%X))%*%t(X)%*%y
return(beta)
}


```

# Task 1c)
Build a function that computes the model residuals. Refer to the previous function `estimate.beta` to get an estimate of the slope coefficients. Here you are only allowed to use matrix and scalar operations.
##E = y - y^
```{r , echo=T}
estimate.residual <- function(X, y){
 res <- (X%*%estimate.beta(X, y)-y)
return(res)
}

```

# Task 1d) 
Build a function that computes $R^2$, i.e. the estimated proportion of variance of $Y$ that is explained by the covariates in your model. Refer to `estimate.residual` to get model residuals. Here you are only allowed to use matrix and scalar operations.
```{r , echo=T}
estimate.R2 <- function(X, y){
R2 <- 1- (var(estimate.residual(X,y))/var(y))
return(R2)
}

```

## Part two: Linear regression practice

# Task 2a)
Now use the lm()-command to fit the same regression model as in Task 1. Refer to the ` guns.data` dataset directly instead of using the matrices ` X` and ` y`.
```{r, echo=TRUE}

lm.fit2a <- lm(y ~ pop.l + avginc + shall + mur.l + rob.l, guns.data)


```

# Task 2b)
Least squares regression coefficients can be extracted from the fitted model ` lm.fit2a ` by using the `coef()`-command. Save the coefficients as a new object. Use your function from task 1b) to get manually constructed least squares estimates. Then, calculate the sum of squared differences between the elements of the two coefficient vectors to confirm that they are practically identical.

```{r, echo=TRUE}
lm.coef2b     <- coef(lm.fit2a)
manual.coef1b <- estimate.beta(X,y)
diff.beta     <- sum((lm.coef2b-manual.coef1b)^2)
print(diff.beta)

```  

# Task 2c)
Model residuals can be extracted from objects created by `lm` using the `residuals()` function. Obtain the model residuals of the regression from task 2a in this way. Additionally, residuals are saved inside the ` lm.coef2b` object. Report the names of all objects within ` lm.coef2b` and calculate the sum of squared differences between the residuals you find there and the residuals that you extracted using `residuals()`.

``` {r, echo=TRUE}
lm.res2c <- residuals(lm.fit2a)
names <- names(lm.coef2b)

diff.res <- sum((lm.res2c-lm.fit2a$residuals)^2)
print(diff.res)

```

# Task 2d)
In order to obtain fitted values, we can use the `predict()`. Do this. The data for which we predict here is the same data used for model training. Accordingly, only need to specify one argument (i.e. input) for `predict()`. 

```{r, echo=TRUE}
lm.pred2d = predict(lm.fit2a)

```

# Task 2e)
A good prediction model for violent crime rates should capture all systematic patterns in the variation of this variable. A simple, but very effective way of finding out whether this is the case is to look at residual plots. If model residuals look like more than just pure noise, then there must be patterns left that we can exploit. Begin by plotting the model residuals from Task 2c (y-axis) against the fitted values from Task 2d (x-axis). Are there remaining patterns in the data?

``` {r, echo=TRUE}
library(ggplot2)
figure2e <- ggplot(data = guns.data) +         # opens plot surface
              geom_point(mapping = aes (x = lm.pred2d, y = lm.res2c)) + # adds scatter plot
              geom_smooth(mapping = aes (x = lm.pred2d, y = lm.res2c), 
                          formula = y~x, se=FALSE,method='loess', col='red') # adds "a fitted smooth curve"
print(figure2e)
rem_patterns2e <- "No remaining pattern is to be found in the variation of the violent crime rates variable. The residual looks like noise." 
```


# Task 2f)
Let us proceed with another plot that should highlight an obvious source of unaccounted patterns in the data. Plot the model residuals against ` stateid`. What do you see?

``` {r, echo=TRUE}

figure2f <- ggplot(data = guns.data) + 
              geom_point(mapping=aes(x = stateid, y = lm.res2c))
print(figure2f)

whatIsee2f <- "You see that in some states the observed violent crime is higher than the predicted violent crime (where the res < 0), whilst in other states, the predicted crime is higher than the obserbed crime (where the res > 0). 'stateid' shows a pattern, but is not included in the model. The model can therefore be underspecified, therefore 'stateid' should be included in the model."

```


# Task 2g) 
`stateid` is a variable that want to add to our model specification in some form. Before doing this, use the `summary()` command to get some descriptive statistics this variable in ` guns.data`. You will see that a mean and a median are reported. Hence, as what type of variable is `stateid` apparently seen by R? Would it make sense to add this variable into our model from Task 2a) as it currently is? Why or why not?

```{r, echo=TRUE}
summary2g <- summary(guns.data$stateid)
print(summary2g)

typeofvarb2g <- "Stateid is seen as a numeric variable and is seen as an integer by R. This can also bee seen in the descriptive statistics, where the meadian is an integer value, as well via the typeof() command."
in_regmodel2g <- "It does not make sense to add the variable in our model. 'stateid' is a categorical variable. In R it would be stored in a factor, which normally have character values linked to the integer values. The states are represented by each number, and should be treated as a factor."

```

# Task 2h)
The way in which R treats a specific variable can change considerably if we encode it as a factor variable. Hence, replace the variable `stateid` in `guns.data` with a version if itself that is encoded as factor variable. Use the `factor()` command for that. Next, get the summary statistics of this modified variable. What has changed?

```{r, echo=TRUE}
guns.data$stateid <- factor(guns.data$stateid)
summary2h <- summary(guns.data$stateid)
print(summary2h)
whatchanged2h <- "It does not show the statistics anymore with the median and mean, but now prints the ids of the states. They are all still integer values, but the class changed from integer to factor."
```

# Task 2i)
Estimate the regression model from Task 2a with factor variable `state_id` as an additional regressor. Use the `summary()` command to report a summary of the regression results. How has `lm()` included `stateid` into the model?

```{r, echo=TRUE}
lm.fit2i <- lm(y ~ pop.l + avginc + shall + mur.l + rob.l + stateid, guns.data)
summary2i <- summary(lm.fit2i)
print(summary2i$coefficients[1:15,])

#howincluded2i <- "It shows that the factors of stateid where taken in the regression in the form of dummyvariables, where every state is a dummy variable (Except stateid1, which is taken into the intercept)."

```

# Task 2j)
The regression results in Task 2i) look the way they do because the ` lm()` command conveniently transforms the factor variable `stateid` into numerical variables before fitting the model. In particular, the `model.matrix()` command is automatically used to arrive at a set of regressors that one can directly feed into a least squares estimation routine. Some important R-commands are less convenient and require you to transform the predictors yourselves. In order to prepare for this situation, use the `model.matrix()` command manually with the same model specification as in Task 2i to get the set of predictors internally generated by `lm()`. Inspect the resulting matrix (e.g. using the `View()` command)  and describe in how far it differs from the variables that you specified.
```{r, echo=TRUE}
predictors2j   <- model.matrix(lm(y ~ pop.l + avginc + shall + mur.l + rob.l + stateid, guns.data))
#howxmatdiffers2j <- "Model.matrix() automatically created dummy variables. In the lm() function, it had to be done manually by changing it to a factor variable. The other predictors did not change when using model.matrix(). It differs in the way that they really became a dummy variably (0 or 1), the model only get affected where the observation is from (the 1)"

```

# Task 2k)
The set of predictors created in Task 2j allows you to use the set of functions for fitting a linear regression model that you wrote in Part 1 of this assignment. We will confirm this by using the ` estimate.R2` function written in Task 1c. Use the matrices ` y` and `predictors 2j` to obtain the R2 of the model specification of Tasks 2i-j. Additionally, use the matrices `y` and ` X` to get an R2 for the model in Task 2a. How did inclusion of `stateid` affect the capability of a linear regression to explain variation in violent crime rates in the sample used for fitting the model?

```{r, echo=TRUE}
lm.R2_withstate2k   <- estimate.R2(predictors2j,y)
lm.R2_nostate2k     <- estimate.R2(X,y)
print(c(lm.R2_withstate2k,lm.R2_nostate2k ))
effect_of_stateid2k <- "The R2 is better when stateid is included (from 0.855 -> 0.965), meaning that the data with the stateid variable included fits the regression model better then it would not have been included. "

```